{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d32cbde-a4c4-43dc-82d9-a40f51558f88",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "`docker compose build && docker compose -d up` to run the postgres and pgadmin containers\n",
    "\n",
    "If psql is installed locally, can connect to the container since the 5432 port is exposed:\n",
    "\n",
    "```sh\n",
    "psql \\\n",
    "    -U postgres \\\n",
    "    -h localhost \\\n",
    "    -p 5432\n",
    "```\n",
    "\n",
    "after which the shell will prompt for password that's set in `.env`\n",
    "\n",
    "\n",
    "## table edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec83a9fc-8bb8-4ced-8743-a307bac6635f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b5d0ada-7821-42b6-878d-63ae1f7f99bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcb4e7e8-03d2-43fc-9b91-b061bb527c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "user = 'postgres'\n",
    "pw = getpass()\n",
    "host = 'localhost'\n",
    "port = '5432'\n",
    "db_name = 'enceladus'\n",
    "conn_str = f'postgresql://{user}:{pw}@{host}:{port}/{db_name}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b558713-3aee-463c-b43d-dffb7f6acf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql $conn_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171011e8-63f5-4f3c-81c3-d40003ab3ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "drop table if exists enceladus;\n",
    "create table enceladus(\n",
    "    id serial primary key,\n",
    "    the_date date,\n",
    "    title varchar(100),\n",
    "    description text\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474c0e0b-0515-4f64-8af3-c670fd6e311b",
   "metadata": {},
   "source": [
    "- `drop table if exists` reduces complication when it comes to errors\n",
    "- `serial` is an always increasing unique sequence to use as primary key\n",
    "    - shorthand from psql\n",
    "    - ANSI sql may use `create sequence id_sq` and then get `nextval('id_sq')` as the primary key\n",
    "- primary key in ansi: `add constraint enceladus primary key (id);`\n",
    "\n",
    "## master schedule\n",
    "\n",
    "Due to budget cuts, each cassini flyby could only operate one or two sensors instead of the originally planned ensemble. Thus the master schedule was borne, to plan the entire operation down to the second.\n",
    "\n",
    "## Importing CSVs\n",
    "\n",
    "- correct typing - each column must have defined type\n",
    "- completeness - not every row may be complete\n",
    "- accuracy - even if each field was filled and had the correct typing, they might not make sense in context; can't have negative kelvins.\n",
    "\n",
    "Importing CSVs or any other data sources must define the protocol for when any one of those criteria are not met, and must require input from stakeholders, e.g. data producer and data consumer\n",
    "\n",
    "### Mechanics\n",
    "\n",
    "start simple, until it doesn't work. In order of complexity:\n",
    "\n",
    "- shell scripts/Make files\n",
    "- python's pandas for more complex functions\n",
    "- kafka for streaming?\n",
    "\n",
    "Import everything as _text_ first; get the data in db, _then_ get the typing right\n",
    "\n",
    "`COPY FROM` reads file from disk to database\n",
    "\n",
    "*Idempotency* must be maintained so that the pipeline can be repeated while arriving at the same result; loading a table should not add a new table the next time\n",
    "\n",
    "## Idempotency\n",
    "\n",
    "`build.sql` contains our load script, where we load the csv entirely as text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73622938-a1e4-49f2-8911-a5bc060c44a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- build.sql part 0\n",
    "create schema if not exists import;\n",
    "drop table if exists import.master_plan;\n",
    "\n",
    "-- build.sql part 1\n",
    "drop table if exists master_plan;\n",
    "create table master_plan(\n",
    "    start_time_utc text,\n",
    "    ... text,\n",
    "    ...,\n",
    ");\n",
    "-- build.sql part 2\n",
    "COPY master_plan\n",
    "FROM 'path/to/master-plan.csv' -- note single quotes\n",
    "-- delimiter: ','; there is header row; csv type\n",
    "WITH DELIMITER ',' HEADER CSV;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57084f75-2fb7-4549-ac7f-f6528b3f40bb",
   "metadata": {},
   "source": [
    "1. create schema for better organization\n",
    "2. creates the empty table with all text types\n",
    "1. load our csv with COPY FROM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32568c8-ddf5-44d7-8f93-acea8b0e0afa",
   "metadata": {},
   "source": [
    "Next we define the _schema_, a de facto namespace feature. Namespace hierarchy in postgres goes:\n",
    "\n",
    "- cluster - set of servers\n",
    "- database\n",
    "- schemas\n",
    "  - default schema is `public`\n",
    "- tables, views, functions; all fall under a schema\n",
    "  - in bigquery this is called _dataset_\n",
    "\n",
    "so much like we don't commit directly to `main` in git, we create a schema for our raw text table: `create schema if not exists import;`\n",
    "\n",
    "Execute in psql: `psql enceladus -f build.sql`\n",
    "\n",
    "Confirm table with `select * from import.master_plan limit 5;`, don't forget the `;`\n",
    "\n",
    "## Make\n",
    "\n",
    "makefile consists of these components\n",
    "\n",
    "- target - top level names for things you want to happen\n",
    "- recipe - commands under `target` that accomplishes the things you want to happen\n",
    "  - must be indented with _tab_, new spaces, as many editors are wont to do\n",
    "- prerequisite - each target may have a pre-req, which are other targets that needs to happen first\n",
    "- variables - could be assigned at the top to parametrize the makefile\n",
    "\n",
    "Common targets:\n",
    "\n",
    "- all: default target; executed if no target specified when calling `make`\n",
    "- clean: teardown, removing build artifacts and cleaning out build dir. Deletes `build.sql`; works if we use our makefile to build a new `build.sql` each time it's called\n",
    "- .PHONY: not really sure\n",
    "\n",
    "Parametrize\n",
    "\n",
    "- `${CURDIR}` returns where `make` is being called, which is useful since psql requires absolute paths when specifying the `build.sql` location\n",
    "\n",
    "Starting makefile:\n",
    "\n",
    "```make\n",
    "DB=enceladus\n",
    "BUILD=${CURDIR}/build.sql\n",
    "SCRIPTS=${CURDIR}/scripts\n",
    "CSV='${CURDIR}/data/master_plan.csv'\n",
    "MASTER=$(SCRIPTS)/import.sql\n",
    "NORMALIZE = $(SCRIPTS)/normalize.sql\n",
    "\n",
    "all: normalize\n",
    "    psql $(DB) -f $(BUILD)\n",
    "\n",
    "master:\n",
    "    @cat $(MASTER) >> $(BUILD)\n",
    "\n",
    "import: master\n",
    "    @echo \"COPY import.master_plan FROM\n",
    "$(CSV) WITH DELIMITER ',' HEADER CSV;\" >> $(BUILD)\n",
    "\n",
    "normalize: import\n",
    "    @cat $(NORMALIZE) >> $(BUILD)\n",
    "\n",
    "clean:\n",
    "    @rm -rf $(BUILD)\n",
    "```\n",
    "\n",
    "`make clean && make` will now build the script from scratch, and re-run it\n",
    "\n",
    "Make  also allows us to compartmentalize the sql commands\n",
    "\n",
    "- import.sql - create import schema and load csv\n",
    "- normalize.sql - split the raw imported table into whatever smaller tables we need"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b888bc6-468a-42a3-8aab-41044df2661d",
   "metadata": {},
   "source": [
    "## Database metadata\n",
    "\n",
    "Search for\n",
    "\n",
    "- `table_schema`\n",
    "- `table_name`\n",
    "- `column_name`\n",
    "- `data_type`\n",
    "\n",
    "from `information_schema.columns` for metadata\n",
    "\n",
    "Postgres keeps internal metadata in these schema:\n",
    "\n",
    "- `information_schema`\n",
    "- `pg_catalog`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634f9eb9-0d6f-4b71-ac95-38b3e8155b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT \n",
    "   distinct table_name, table_schema, column_name, data_type\n",
    "   \n",
    "FROM \n",
    "   information_schema.columns\n",
    "WHERE\n",
    "    table_schema not in ('information_schema', 'pg_catalog')\n",
    "order by table_name;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
